{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70c76a6-6076-4d0a-9a43-e0df9e2446b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.replication=1 -cp -f data/*.jsonl hdfs://nn:9000/\n",
    "!hdfs dfs -D dfs.replication=1 -cp -f data/*.csv hdfs://nn:9000/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793b538b-c082-42be-9787-8233e394b096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/20 14:40:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"1G\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501ff476-24fd-4aab-b14c-db429293e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "problems_df = spark.read.json(\"hdfs://nn:9000/problems.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26d9292-f351-4b53-8538-f5cfc9ad6a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|cf_contest_id|cf_index|cf_points|cf_rating|        cf_tags|difficulty|generated_tests|is_description_translated|memory_limit_bytes|                name|private_tests|problem_id|public_tests|source|time_limit|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|          322|       A|    500.0|     1000|            [0]|         7|             93|                    false|         256000000|322_A. Ciel and D...|           45|         1|           2|     2|         1|\n",
      "|          760|       D|   1000.0|     1600|         [1, 2]|        10|             51|                    false|         256000000|  760_D. Travel Card|            4|         2|           2|     2|         2|\n",
      "|          569|       E|   1500.0|     2600|         [3, 0]|        11|             99|                    false|         256000000| 569_E. New Language|           17|         3|           3|     2|         2|\n",
      "|          447|       B|   1000.0|     1000|         [0, 4]|         8|            100|                    false|         256000000|447_B. DZY Loves ...|           13|         4|           1|     2|         1|\n",
      "|         1292|       B|    750.0|     1700|[5, 6, 7, 0, 4]|         8|             91|                    false|         256000000|1292_B. Aroma's S...|          131|         5|           3|     2|         1|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6444ef-3a3b-42d1-b205-648e0f848dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(problems_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3058e85-d8a1-4dd3-afe9-e8f3f32a7421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems_df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b98b7e7-068b-4617-b470-21931a6408f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "problems_df=spark.read.format(\"JSON\").load(\"hdfs://nn:9000/problems.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5be918d-0265-4e6e-87be-d4515c2c727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|cf_contest_id|cf_index|cf_points|cf_rating|        cf_tags|difficulty|generated_tests|is_description_translated|memory_limit_bytes|                name|private_tests|problem_id|public_tests|source|time_limit|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|          322|       A|    500.0|     1000|            [0]|         7|             93|                    false|         256000000|322_A. Ciel and D...|           45|         1|           2|     2|         1|\n",
      "|          760|       D|   1000.0|     1600|         [1, 2]|        10|             51|                    false|         256000000|  760_D. Travel Card|            4|         2|           2|     2|         2|\n",
      "|          569|       E|   1500.0|     2600|         [3, 0]|        11|             99|                    false|         256000000| 569_E. New Language|           17|         3|           3|     2|         2|\n",
      "|          447|       B|   1000.0|     1000|         [0, 4]|         8|            100|                    false|         256000000|447_B. DZY Loves ...|           13|         4|           1|     2|         1|\n",
      "|         1292|       B|    750.0|     1700|[5, 6, 7, 0, 4]|         8|             91|                    false|         256000000|1292_B. Aroma's S...|          131|         5|           3|     2|         1|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a42345-d768-44a7-9ab3-2321a1d0c560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(problems_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2838804f-bd0a-4daa-81a0-6bd174b49041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[20] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems_df.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81ebaae-bb00-4d7d-984b-80a577c1bc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "filter_count = problems_df.rdd.filter(lambda row: row.cf_rating >= 1600 and row.private_tests and \"_A.\" in row.name ).count()\n",
    "filter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78487c1e-9352-44f0-8409-46114cdea1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "from pyspark.sql.functions import col, expr\n",
    "problems_df.filter(expr(\"cf_rating >=1600\")).filter(expr(\"private_tests > 0\")).filter(expr(\"contains(name,'_A.')\")).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d161e8-8af2-483e-996f-d7b88d6bad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 14:40:48 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/03/20 14:40:48 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/03/20 14:40:51 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/03/20 14:40:51 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.4\n",
      "25/03/20 14:40:52 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/03/20 14:40:55 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/03/20 14:40:55 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/03/20 14:40:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/03/20 14:40:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "problems_df.write.mode(\"overwrite\").saveAsTable(\"problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da1b6f4e-46eb-444c-bfd0-d3fe1fd0ed02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.table(\"problems\")\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b8b152e-6cf7-4c90-a0a4-f8b67188005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM problems\n",
    "WHERE cf_rating >= 1600 \n",
    "AND private_tests > 0\n",
    "AND contains(name,'_A.')\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6305f8-b4c6-4976-b8e8-ee2eafa8d3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[language#428], functions=[count(1)])\n",
      "   +- HashAggregate(keys=[language#428], functions=[partial_count(1)])\n",
      "      +- FileScan parquet spark_catalog.default.solutions[language#428] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://nn:9000/user/hive/warehouse/solutions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<language:string>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q4\n",
    "solutions_df=spark.read.format(\"JSON\").load(\"hdfs://nn:9000/solutions.jsonl\")\n",
    "(\n",
    "    solutions_df\n",
    "    .write\n",
    "    .bucketBy(4, \"language\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"solutions\")\n",
    ")\n",
    "spark.sql(\"\"\"\n",
    "SELECT language, COUNT(*)\n",
    "FROM solutions\n",
    "GROUP BY language\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8d408a-f153-4156-8cff-e7b3ab942279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problems': False,\n",
       " 'solutions': False,\n",
       " 'languages': True,\n",
       " 'problem_tests': True,\n",
       " 'sources': True,\n",
       " 'tags': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "import pandas as pd\n",
    "csvs = [\"languages\", \"problem_tests\", \"sources\", \"tags\"]\n",
    "for csv in csvs:\n",
    "    table = spark.read.format(\"csv\").option(\"header\", True).load(f\"hdfs://nn:9000/{csv}.csv\")\n",
    "    table.createOrReplaceTempView(csv)\n",
    "\n",
    "ans = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "#copied from \"pandas use column values as key as value\" search ai\n",
    "ans.set_index('tableName')['isTemporary'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66d5489c-3c4c-4703-8012-2a665c6affc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10576"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "'''Join the solutions table with the problems table using an inner join on the problem_id column. \n",
    "Note that the source column in problems is an integer. Join this column with the source_id column \n",
    "from the sources CSV. Find the number of correct PYTHON3 solutions from CODEFORCES.\n",
    "'''\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM solutions\n",
    "INNER JOIN problems ON solutions.problem_id = problems.problem_id\n",
    "INNER JOIN sources ON problems.source = sources.source\n",
    "WHERE is_correct = true AND solutions.language = \"PYTHON3\" AND sources.source_name = \"CODEFORCES\"\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c696e4f-f5c8-4d5d-8dfc-146b50c22ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# SELECT *\n",
    "# FROM sources\n",
    "# \"\"\").limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96422142-d35b-4313-81b4-aa889574f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# SELECT *\n",
    "# FROM solutions\n",
    "# \"\"\").limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34736254-51fb-43c7-bc51-0160ecd9daf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Medium': 5768, 'Hard': 2396, 'Easy': 409}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "'''\n",
    "<= 5 is Easy\n",
    "<= 10 is Medium\n",
    "Otherwise Hard\n",
    "'''\n",
    "difficulties = spark.sql(\"\"\"\n",
    "SELECT difficulty,\n",
    "CASE \n",
    "    WHEN difficulty <= 5 THEN 'Easy'\n",
    "    WHEN difficulty <= 10 THEN 'Medium'\n",
    "    ELSE 'Hard'\n",
    "END AS levels\n",
    "FROM problems\n",
    "\"\"\").toPandas()\n",
    "counts = difficulties['levels'].value_counts().to_dict()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16fa66b5-ab27-42d8-9b2f-52746bf8963c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.028772830963134766, 0.02396845817565918, 0.024501323699951172]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q8\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "problemTests = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM problem_tests\n",
    "WHERE is_generated = false\n",
    "\"\"\")\n",
    "import time\n",
    "ts = []\n",
    "t0 = time.time()\n",
    "problemTests.select(mean(col(\"input_chars\")))\n",
    "problemTests.select(mean(col(\"output_chars\")))\n",
    "t1 = time.time()\n",
    "time1 = t1-t0\n",
    "# print(time)\n",
    "ts.append(time1)\n",
    "newProblemTests = problemTests.cache()\n",
    "t2 = time.time()\n",
    "newProblemTests.select(mean(col(\"input_chars\")))\n",
    "newProblemTests.select(mean(col(\"output_chars\")))\n",
    "t3 = time.time()\n",
    "time2 = t3-t2\n",
    "# print(time)\n",
    "ts.append(time2)\n",
    "t4 = time.time()\n",
    "newProblemTests.select(mean(col(\"input_chars\")))\n",
    "newProblemTests.select(mean(col(\"output_chars\")))\n",
    "t5 = time.time()\n",
    "time3 = t5-t4\n",
    "# print(time)\n",
    "ts.append(time3)\n",
    "problemTests.unpersist()\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ef6ac4c-af0c-4231-b0d4-418d6bb78128",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train dataset: cf_rating is >0, and problem_id in an EVEN number\n",
    "test dataset: cf_rating is >0, and problem_id in an ODD number\n",
    "missing dataset: cf_rating is 0\n",
    "'''\n",
    "train = spark.sql(\"\"\"\n",
    "SELECT sources.source, problems.source, source_name, problem_id, cf_rating, difficulty, time_limit, memory_limit_bytes\n",
    "FROM problems\n",
    "INNER JOIN sources ON problems.source = sources.source\n",
    "WHERE sources.source_name = \"CODEFORCES\" and cf_rating > 0 and problem_id % 2 = 0\n",
    "\"\"\")\n",
    "test = spark.sql(\"\"\"\n",
    "SELECT sources.source, problems.source, source_name, problem_id, cf_rating, difficulty, time_limit, memory_limit_bytes\n",
    "FROM problems\n",
    "INNER JOIN sources ON problems.source = sources.source\n",
    "WHERE sources.source_name = \"CODEFORCES\" and cf_rating > 0 and problem_id % 2 = 1\n",
    "\"\"\")\n",
    "missing = spark.sql(\"\"\"\n",
    "SELECT sources.source, problems.source, source_name, problem_id, cf_rating, difficulty, time_limit, memory_limit_bytes\n",
    "FROM problems\n",
    "INNER JOIN sources ON problems.source = sources.source\n",
    "WHERE sources.source_name = \"CODEFORCES\" and cf_rating = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d486f1dc-cf5b-4b01-9567-754af3bfb06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5929835263198762"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "from pyspark.ml.regression import DecisionTreeRegressor       \n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel  \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline     \n",
    "from pyspark.ml.pipeline import PipelineModel \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"difficulty\", \"time_limit\", \"memory_limit_bytes\"], outputCol=\"features\")\n",
    "# va.transform(train).limit(5).show()\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"cf_rating\")\n",
    "pipe = Pipeline(stages=[va, dt])\n",
    "model = pipe.fit(train)\n",
    "r2score = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"cf_rating\", metricName=\"r2\")\n",
    "r2score.evaluate(model.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ddb4788-7e82-4591-a3f9-24bb313e0b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1887.9377431906614, 1893.1106471816283, 1950.4728638818783)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "avgs = []\n",
    "avgTrain = train.select(mean(col(\"cf_rating\")))\n",
    "#copied from https://stackoverflow.com/questions/31961971/spark-extracting-single-value-from-dataframe\n",
    "avgs.append(avgTrain.head()[0])\n",
    "avgTest = test.select(mean(col(\"cf_rating\")))\n",
    "avgs.append(avgTest.head()[0])\n",
    "# avgs.append(avgTrain)\n",
    "\n",
    "\n",
    "predMissing = model.transform(missing)\n",
    "avgMissing = predMissing.select(mean(col(\"prediction\")))\n",
    "avgs.append(avgMissing.head()[0])\n",
    "tuple(avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ce685-ac8f-4c2b-9966-6d3dbd1caef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
